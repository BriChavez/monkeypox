{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quinn\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dataset_df = spark.read.csv('Monkey_Pox_Cases_Worldwide.csv', header=True)\n",
    "daily_df = spark.read.csv(\n",
    "    'Daily_Country_Wise_Confirmed_Cases.csv', header=True)\n",
    "\n",
    "total_df = dataset_df.select(\n",
    "    dataset_df['Country'], dataset_df['Confirmed_Cases'])\n",
    "\n",
    "dataset_df = total_df.join(daily_df, ['Country'], how='left')\n",
    "\n",
    "\n",
    "def dashes_to_underscores(s):\n",
    "    return s.replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "dataset_df = dataset_df.transform(\n",
    "    quinn.with_columns_renamed(dashes_to_underscores))\n",
    "\n",
    "dataset_df.write.option('header', True) \\\n",
    "                .csv('../data/daily_cases.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# turn the spark df into pandas df\n",
    "pd_pox = dataset_df.toPandas()\n",
    "\n",
    "# drop the silly enumerated index and replace it with count=rty names\n",
    "pd_pox.set_index('Country', drop=True, inplace=True)\n",
    "# this will take our row and column indices and flip 'em\n",
    "what = pd_pox.transpose()\n",
    "\n",
    "\n",
    "# set df data types to floats and then into ints\n",
    "what = what.astype(float).astype('Int64')\n",
    "# see if we have any null values\n",
    "\n",
    "# replace them with 0\n",
    "what = what.fillna(0)\n",
    "\n",
    "# pd.isnull(what).sum()\n",
    "\n",
    "what.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['England', 'Portugal', 'Spain', 'United States', 'Canada', 'Sweden',\n",
       "       'Italy', 'France', 'Belgium', 'Australia', 'Germany', 'Netherlands',\n",
       "       'Israel', 'Switzerland', 'Greece', 'Austria', 'Argentina', 'Denmark',\n",
       "       'Morocco', 'Slovenia', 'Scotland', 'Czech Republic',\n",
       "       'United Arab Emirates', 'Finland', 'Wales', 'Northern Ireland', 'Sudan',\n",
       "       'Bolivia', 'Iran', 'Ecuador', 'Malta', 'Ireland', 'Mexico', 'Pakistan',\n",
       "       'French Guiana', 'Thailand', 'Peru', 'Brazil', 'Malaysia', 'Hungary',\n",
       "       'Norway', 'Paraguay', 'Costa Rica', 'Gibraltar', 'Mauritius', 'Haiti',\n",
       "       'Uruguay', 'Latvia', 'Cayman Islands', 'Kosovo', 'Turkey', 'Bahamas',\n",
       "       'Ghana', 'India', 'Iceland', 'Poland', 'Bangladesh', 'Uganda',\n",
       "       'Cambodia', 'Malawi', 'Venezuela', 'Romania', 'Georgia', 'Slovakia',\n",
       "       'Luxembourg', 'Nepal', 'Chile', 'Serbia', 'Lebanon', 'South Korea',\n",
       "       'Singapore', 'South Africa', 'Taiwan', 'Colombia', 'Croatia',\n",
       "       'Bulgaria', 'Somalia', 'Zambia', 'Fiji', 'Benin', 'Estonia',\n",
       "       'Puerto Rico'],\n",
       "      dtype='object', name='Country')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# turn the spark df into pandas df\n",
    "pd_pox = dataset_df.toPandas()\n",
    "# type(pd_pox)\n",
    "\n",
    "# pd_pox.set_index('Country', drop = True, inplace = True)\n",
    "what = pd_pox.pivot_table(columns = 'Country', index= 'Confirmed_Cases', fill_value = 0)\n",
    "# pd_pox.info()\n",
    "# pd_pox.describe\n",
    "pd.isnull(what).max()\n",
    "# pd_pox.index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'what' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3436/2911771619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m total_cases = pd_pox.pivot_table(\n\u001b[1;32m      5\u001b[0m     columns='Country', values='Confirmed_Cases', fill_value=0)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'what' is not defined"
     ]
    }
   ],
   "source": [
    "new_df = what.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "total_cases = pd_pox.pivot_table(\n",
    "    columns='Country', values='Confirmed_Cases', fill_value=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_cases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3436/705494250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# total_cases.describe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# total_cases.index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_cases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# # plt.xLabel('Country')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# # plt.yLabel('Confirmed_Cases')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_cases' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# new_df = what.select_dtypes(include=[np.number])\n",
    "# total_cases.describe\n",
    "# total_cases.index\n",
    "plt.hist(total_cases.us)\n",
    "# # plt.xLabel('Country')\n",
    "# # plt.yLabel('Confirmed_Cases')\n",
    "# # plt.title('yikes')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7f436e329750>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/fossa/data/dont_worry_about_it/dearHenry.json\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "# dataset_id = f\"{client.project}.monkeypox\"\n",
    "# bucket = 'hole-in-the'\n",
    "# spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# dataset_df.write.format(\"bigquery\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"temporaryGcsBucket\", \"databricks-bigquery-temp\") \\\n",
    "#     .option(\"table\", \"dearliza.monkeypox_data.monkeypox\") \\\n",
    "#     .save()\n",
    "\n",
    "dataset_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"hole-in-the\") \\\n",
    "    .option(\"table\", \"monkeypox.monkeypox_table\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Instantiates a client\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/fossa/data/dont_worry_about_it/dearHenry.json\"\n",
    "client = storage.Client()\n",
    "\n",
    "\n",
    "# Retrieve an existing bucket\n",
    "# https://console.cloud.google.com/storage/browser/hole-in-the/\n",
    "bucket = client.get_bucket('hole-in-the')\n",
    "# Then do other things...\n",
    "blob = bucket.blob('daily_cases.csv')\n",
    "blob.upload_from_filename('../data/Daily_Country_Wise_Confirmed_Cases.csv')\n",
    "blob = bucket.blob('cases_worldwide.csv')\n",
    "blob.upload_from_filename('../data/Monkey_Pox_Cases_Worldwide.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd \n",
    "import pandas_gbq\n",
    "\n",
    "daily_cases = pd.read_csv('../data/Daily_Country_Wise_Confirmed_Cases.csv')\n",
    "# cases_worldwide = pd.read_csv('../data/Monkey_Pox_Cases_Worldwide.csv')\n",
    "\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_id = f\"{client.project}.monkeypox\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = 'us'\n",
    "dataset = client.create_dataset(dataset, exists_ok = True, timeout =100)\n",
    "project_id = 'dearliza'\n",
    "table_id = 'monkeypox.monkeypox_data'\n",
    "\n",
    "pandas_gbq.to_gbq(daily_cases, table_id, project_id = project_id, if_exists = 'replace', api_method = 'load_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5017/3272153821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from matplotlib.pylab import rcParams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 20, 10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5176a492cff03d096a1eeda30155d13c90600b4ba6733f04f70e8262b03a11c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
