{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/08 16:01:27 WARN Utils: Your hostname, fossa-dsa-001 resolves to a loopback address: 127.0.1.1; using 192.168.0.111 instead (on interface wlp3s0)\n",
      "22/07/08 16:01:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/08 16:01:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/08 16:01:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"JESS\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import date\n",
    "import quinn\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "def dashes_to_underscores(s):\n",
    "    return s.replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#create df\n",
    "daily_df = spark.read.csv(\n",
    "    'Daily_Country_Wise_Confirmed_Cases.csv', header=True)\n",
    "#perform transformations\n",
    "dataset_df = daily_df.transform(\n",
    "    quinn.with_columns_renamed(dashes_to_underscores))\n",
    "#create pandas df and set index\n",
    "pandas_df = daily_df.toPandas()\n",
    "pandas_df.set_index('Country', inplace=True)\n",
    "#transpose and reset index\n",
    "transpose_df = pandas_df.transpose()\n",
    "transpose_df.reset_index(inplace=True)\n",
    "#create spark df\n",
    "spark_df = spark.createDataFrame(transpose_df)\n",
    "spark_df = spark_df.withColumnRenamed('index', 'Date')\n",
    "\n",
    "# spark_df.write.csv(f\"Daily_CC_{date.today()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.write.csv(f\"Daily_CC_{date.today()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BRI\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# turn the spark df into pandas df\n",
    "pd_pox = spark_df.toPandas()\n",
    "\n",
    "# pd_pox = pd_pox.fillna(0)\n",
    "pd.isnull(pd_pox).sum()\n",
    "\n",
    "# drop the silly enumerated index and replace it with country names\n",
    "pd_pox.set_index('Date', drop=True, inplace=True)\n",
    "# this will take our row and column indices and flip 'em\n",
    "# flipped_df = pd_pox.transpose()\n",
    "# pd_pox.Confirmed_Cases = pd.to_numeric(pd_pox.Confirmed_Cases)\n",
    "                # ('Confirmed_Cases', 'string'),\n",
    "                # ('Suspected_Cases', 'string'),\n",
    "                # ('Hospitalized', 'string'),\n",
    "                # ('Travel_History_Yes', 'string'),\n",
    "                # ('Travel_History_No', 'string'),\n",
    "                # ('2022-07-01', 'string'),\n",
    "\n",
    "# print(type(pd_pox.2022-07-01))\n",
    "# )\n",
    "# \n",
    "# set df data types to floats and then into ints\n",
    "pd_pox = pd_pox.astype(float).astype('Int64')\n",
    "# # # see if we have any null values\n",
    "# pd.isnull(pd_pox).sum()\n",
    "# # # replace them with 0\n",
    "# flipped_df = flipped_df.fillna(0)\n",
    "# pd_pox.dtypes\n",
    "# pd.isnull(flipped_df).sum()\n",
    "# pd_pox.columns\n",
    "pd_pox.to_csv('../data/daily_cases.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5176a492cff03d096a1eeda30155d13c90600b4ba6733f04f70e8262b03a11c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
